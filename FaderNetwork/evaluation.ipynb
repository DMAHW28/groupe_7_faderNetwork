{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81ea482-ac4b-4219-a28e-c1ec90368d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from Models.fader_network import FaderNetwork\n",
    "import numpy as np\n",
    "from Data.preprocess import load_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0e2da9-b628-43b3-bdec-3a86b37b6a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation du device : cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing in batches: 100%|██████████| 2599/2599 [00:00<00:00, 468940.73it/s]\n",
      "Processing Attribute: 100%|██████████| 2597/2597 [00:00<00:00, 43114.45it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:868: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. Saved a reverse patch to Sequential.patch. Run `patch -p0 < Sequential.patch` to revert your changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:868: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. Tried to save a patch, but couldn't create a writable file Conv2d.patch. Make sure it doesn't exist and your working directory is writable.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:868: SourceChangeWarning: source code of class 'torch.nn.modules.activation.LeakyReLU' has changed. Saved a reverse patch to LeakyReLU.patch. Run `patch -p0 < LeakyReLU.patch` to revert your changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:868: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. Saved a reverse patch to BatchNorm2d.patch. Run `patch -p0 < BatchNorm2d.patch` to revert your changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:868: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. Saved a reverse patch to Linear.patch. Run `patch -p0 < Linear.patch` to revert your changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèles chargés et prêts pour le test.\n"
     ]
    }
   ],
   "source": [
    "# Configuration des paramètres\n",
    "selected_attrs = ['Smiling', 'Male', 'Eyeglasses', 'Young', 'Mouth_Slightly_Open']\n",
    "num_attributes = len(selected_attrs)\n",
    "end = 202599\n",
    "start = 200000\n",
    "index = (start, end)\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Utilisation du device : {device}\")\n",
    "\n",
    "auto_encoder_full_BD_100_epochs = FaderNetwork(attribute_dim=num_attributes, attributes=selected_attrs).to(device)\n",
    "auto_encoder_batch_60K_100_epochs = FaderNetwork(attribute_dim=num_attributes, attributes=selected_attrs).to(device)\n",
    "auto_encoder_batch_50K_50_epochs = FaderNetwork(attribute_dim=num_attributes, attributes=selected_attrs).to(device)\n",
    "\n",
    "(train_loader, valid_loader, test_loader) = load_images(index = index, selected_attrs = selected_attrs, batch_size=32)\n",
    "\n",
    "# On charge le classifier du GitHub de l'article pour évaluer le modèle \n",
    "torch.nn.Module.dump_patches = True\n",
    "clf = torch.load('./classifier256.pth', map_location=torch.device(device), weights_only=False)\n",
    "clf.eval()\n",
    "\n",
    "# On charge le modèle qui à appris sur toute le base de données avec 100 epochs\n",
    "auto_encoder_full_BD_100_epochs.load_state_dict(torch.load('./Models/trained_model/TrainBD_100_full/auto_encoder_epoch_100.pth'))\n",
    "auto_encoder_full_BD_100_epochs.eval()\n",
    "\n",
    "# On charge le modèle qui à appris sur toute le base de données mais avec un batch_size de 60K images et 100 epochs\n",
    "auto_encoder_batch_60K_100_epochs.load_state_dict(torch.load('./Models/trained_model/TrainBD_50_100_2/auto_encoder_epoch_100.pth'))\n",
    "auto_encoder_batch_60K_100_epochs.eval()\n",
    "\n",
    "# On charge le modèle qui à appris sur toute le base de données mais avec un batch_size de 50K images et 50 epochs\n",
    "auto_encoder_batch_50K_50_epochs.load_state_dict(torch.load('./Models/trained_model/TrainBD_50_full_4//auto_encoder_epoch_50.pth'))\n",
    "auto_encoder_batch_50K_50_epochs.eval()\n",
    "\n",
    "print(\"Modèles chargés et prêts pour le test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cdf4ad6-6388-4dd6-84ba-612f38ef5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_classifier(output, attributes, att_position):\n",
    "    \"\"\"\n",
    "    Calcule la précision pour les prédictions du discriminateur latent.\n",
    "    Args:\n",
    "        output (torch.Tensor): Les prédictions du modèle (batch_size, 2*n_attributes).\n",
    "        attributes (torch.Tensor): Les attributs cibles (batch_size, 2*n_attributes).\n",
    "    Returns:\n",
    "        float: Précision totale sur le batch.\n",
    "    \"\"\"\n",
    "    att_accuracies = []\n",
    "    accuracy = 0 # Initialisation de la précision totale\n",
    "    n_cat = 2 # Chaque attribut est encodé sur deux colonnes (codage one hot)\n",
    "    for k, p in zip(range(0, attributes.size(1), n_cat), att_position):\n",
    "        # Sélectionner les prédictions pour l'attribut courant\n",
    "        pred = output[:, p:p + n_cat].argmax(1)\n",
    "        # Obtenir les labels cibles pour cet attribut\n",
    "        y = attributes[:, k:k + n_cat].argmax(1)\n",
    "        # Vérification des correspondances entre prédictions et cibles\n",
    "        accuracy = pred.eq(y.view_as(pred)).sum().item()\n",
    "        att_accuracies.append(accuracy)\n",
    "    return att_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ce2d2b2-b4d1-471f-8f60-5a2a3e21c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_position = [62, 40, 30, 78, 42]\n",
    "att_position_start = [-1, 0, 2, 4, 6, 8]\n",
    "model_clf_acc_real_img = np.zeros(num_attributes)\n",
    "model_clf_acc_gen_img_model_full_bd = np.zeros(num_attributes)\n",
    "model_clf_acc_gen_img_model_60_bd = np.zeros(num_attributes)\n",
    "model_clf_acc_gen_img_model_50_bd = np.zeros(num_attributes)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for l, (X, y) in enumerate(test_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        flip_start = np.random.choice(att_position_start)\n",
    "        if flip_start == -1:\n",
    "            target_gen = 1 - y\n",
    "        else:\n",
    "            target_gen = y.clone()\n",
    "            target_gen[:, flip_start:flip_start+2] = 1 - target_gen[:, flip_start:flip_start+2]\n",
    "            \n",
    "        _, D1 = auto_encoder_full_BD_100_epochs(X, target_gen)\n",
    "        _, D2 = auto_encoder_batch_60K_100_epochs(X, target_gen)\n",
    "        _, D3 = auto_encoder_batch_50K_50_epochs(X, target_gen)\n",
    "        \n",
    "        clf_outputs_1 = clf(D1)\n",
    "        clf_outputs_2 = clf(D2)\n",
    "        clf_outputs_3 = clf(D3)\n",
    "        clf_outputs_real_image = clf(X)\n",
    "        \n",
    "        acc_1 = accuracy_classifier(clf_outputs_1, target_gen, att_position)\n",
    "        acc_2 = accuracy_classifier(clf_outputs_2, target_gen, att_position)\n",
    "        acc_3 = accuracy_classifier(clf_outputs_3, target_gen, att_position)\n",
    "        acc_real_img = accuracy_classifier(clf_outputs_real_image, y, att_position)\n",
    "        \n",
    "        model_clf_acc_gen_img_model_full_bd += np.array(acc_1)\n",
    "        model_clf_acc_gen_img_model_60_bd += np.array(acc_2)\n",
    "        model_clf_acc_gen_img_model_50_bd += np.array(acc_3)\n",
    "        model_clf_acc_real_img += np.array(acc_real_img)\n",
    "        \n",
    "    model_clf_acc_gen_img_model_full_bd /= len(test_loader.dataset)\n",
    "    model_clf_acc_gen_img_model_60_bd /= len(test_loader.dataset)\n",
    "    model_clf_acc_gen_img_model_50_bd /= len(test_loader.dataset)\n",
    "    model_clf_acc_real_img /= len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c902649-24b2-4084-a2c5-3493eff198bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance du classifieur sur les Images Réelles\n",
      "Attribut Smiling accuracy : 93.53846153846153 % \n",
      "\n",
      "Attribut Male accuracy : 97.84615384615385 % \n",
      "\n",
      "Attribut Eyeglasses accuracy : 99.84615384615385 % \n",
      "\n",
      "Attribut Young accuracy : 88.61538461538461 % \n",
      "\n",
      "Attribut Mouth_Slightly_Open accuracy : 93.84615384615384 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance du classifieur sur les Images Réelles\")\n",
    "for name, prob in zip(selected_attrs, model_clf_acc_real_img):\n",
    "    print(f'Attribut {name} accuracy : {prob*100} % \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84d68b4e-2ce7-4d93-9a7c-2cc6196bd0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance du classifieur sur les Images générées avec le modèle auto_encoder_full_BD_100_epochs\n",
      "Attribut Smiling accuracy : 94.0 % \n",
      "\n",
      "Attribut Male accuracy : 88.61538461538461 % \n",
      "\n",
      "Attribut Eyeglasses accuracy : 72.92307692307692 % \n",
      "\n",
      "Attribut Young accuracy : 72.92307692307692 % \n",
      "\n",
      "Attribut Mouth_Slightly_Open accuracy : 83.07692307692308 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance du classifieur sur les Images générées avec le modèle auto_encoder_full_BD_100_epochs\")\n",
    "for name, prob in zip(selected_attrs, model_clf_acc_gen_img_model_full_bd):\n",
    "    print(f'Attribut {name} accuracy : {prob*100} % \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0698b633-81a8-4c73-ad4d-04091fc342d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance du classifieur sur les Images générées avec le modèle auto_encoder_batch_60K_100_epochs\n",
      "Attribut Smiling accuracy : 86.76923076923076 % \n",
      "\n",
      "Attribut Male accuracy : 89.38461538461539 % \n",
      "\n",
      "Attribut Eyeglasses accuracy : 74.92307692307692 % \n",
      "\n",
      "Attribut Young accuracy : 73.6923076923077 % \n",
      "\n",
      "Attribut Mouth_Slightly_Open accuracy : 76.76923076923077 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance du classifieur sur les Images générées avec le modèle auto_encoder_batch_60K_100_epochs\")\n",
    "for name, prob in zip(selected_attrs, model_clf_acc_gen_img_model_60_bd):\n",
    "    print(f'Attribut {name} accuracy : {prob*100} % \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "811b00e2-7625-4a38-b800-5f66ca165fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance du classifieur sur les Images générées avec le modèle auto_encoder_batch_50K_50_epochs\n",
      "Attribut Smiling accuracy : 91.84615384615384 % \n",
      "\n",
      "Attribut Male accuracy : 92.61538461538461 % \n",
      "\n",
      "Attribut Eyeglasses accuracy : 91.07692307692308 % \n",
      "\n",
      "Attribut Young accuracy : 71.07692307692308 % \n",
      "\n",
      "Attribut Mouth_Slightly_Open accuracy : 84.15384615384616 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance du classifieur sur les Images générées avec le modèle auto_encoder_batch_50K_50_epochs\")\n",
    "for name, prob in zip(selected_attrs, model_clf_acc_gen_img_model_50_bd):\n",
    "    print(f'Attribut {name} accuracy : {prob*100} % \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
